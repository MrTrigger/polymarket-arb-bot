# Progress Log

## Project Config
backend: rust
frontend: none
infra: kubernetes

## Project Overview
Polymarket 15-Minute Arbitrage Bot - HFT-optimized trading bot for crypto binary options arbitrage.

### Key Architecture Decisions
- Lock-free hot path using DashMap and atomics
- Zero-copy WebSocket parsing where possible
- Pre-hashed EIP-712 signing for shadow bids (<2ms)
- Fire-and-forget observability (<10ns overhead)
- Separate binaries: poly-collect (data), poly-import (historical), poly-bot (trading)

### External Dependencies
- polymarket-client-sdk: Official Polymarket Rust client (auth, discovery, orders)
- tokio-tungstenite: Custom WebSocket (minimal overhead for Binance)
- dashmap: Lock-free concurrent hash maps
- rust_decimal: Financial math (NEVER use f64 for prices)
- clickhouse: Data storage and replay

### Performance Targets
- WS message to state: <10us
- Arb detection: <1us
- Circuit breaker check: <10ns
- Shadow bid fire: <2ms
- Observability overhead: <10ns

## Codebase Patterns
[Agents add patterns they discover here]

## Priority
1. Phase 1 (poly-collect) is PRIORITY - start collecting data ASAP
2. Phases 2-9 can be developed in parallel while data collects
3. Every day of delay = less backtest data

## Completed Work

---

### [2026-01-12] Task p1-1: Workspace and shared crate setup
Files created:
- Cargo.toml (workspace root with all dependencies)
- crates/poly-common/Cargo.toml
- crates/poly-common/src/lib.rs
- crates/poly-common/src/types.rs

Verification:
- cargo check: PASSED
- cargo clippy -- -D warnings: PASSED
- cargo test: PASSED (5 tests)

Types implemented:
- CryptoAsset (BTC, ETH, SOL, XRP) with binance_symbol() helper
- Side (Buy, Sell) with opposite()
- Outcome (Yes, No) with opposite()
- OrderBookLevel (price, size) - all Decimal
- MarketWindow with window timing helpers
- SpotPrice for Binance trade data
- OrderBookSnapshot and OrderBookDelta for ClickHouse storage

Key decisions:
- Using rust_decimal v1.36 for all financial math
- clickhouse crate v0.13 with Row derive for type-safe inserts
- chrono with serde feature for timestamp handling
- All ClickHouse types derive Row for inserter compatibility

---

### [2026-01-12] Task p1-2: ClickHouse schema and client
Files created:
- crates/poly-common/src/schema.sql
- crates/poly-common/src/clickhouse.rs

Files modified:
- crates/poly-common/src/lib.rs (added clickhouse module export)

Verification:
- cargo clippy -- -D warnings: PASSED
- cargo test: PASSED (8 tests)

Schema tables defined:
- market_windows: ReplacingMergeTree for discovered markets
- spot_prices: MergeTree with 90-day TTL for Binance trades
- orderbook_snapshots: MergeTree with 90-day TTL for periodic captures
- orderbook_deltas: MergeTree with 90-day TTL for incremental updates
- price_history: MergeTree for historical import
- trade_history: ReplacingMergeTree for historical import
- decisions: MergeTree with 180-day TTL for observability

ClickHouseClient features:
- ClickHouseConfig with sensible defaults (max_rows: 10k, max_bytes: 10MB, period: 5s)
- ping() for connection testing
- create_tables() parses and executes schema.sql
- Type-safe inserters for each table type with auto-commit configuration
- Batch insert methods for one-off writes

Key decisions:
- Using Inserter from clickhouse::inserter module (not re-exported at crate root)
- Decimal(18, 8) for all price/quantity columns
- DateTime64(3, 'UTC') for millisecond precision timestamps
- LowCardinality(String) for repeated values (asset, side, action)
- 90-day TTL on operational data, 180-day on decisions

---

### [2026-01-12] Task p1-3: Market discovery for collector
Files created:
- crates/poly-collect/Cargo.toml
- crates/poly-collect/src/lib.rs
- crates/poly-collect/src/discovery.rs
- crates/poly-collect/src/main.rs

Files modified:
- Cargo.toml (added poly-collect to workspace members)

Verification:
- cargo clippy -- -D warnings: PASSED
- cargo test -p poly-collect: PASSED (4 tests)

MarketDiscovery features:
- Fetches active events from Gamma API (https://gamma-api.polymarket.com)
- Filters for 15-minute up/down markets by title keywords
- Detects crypto asset (BTC, ETH, SOL, XRP) from title
- Parses YES/NO token IDs from clobTokenIds JSON string
- Parses strike price from title (e.g., "$100,000")
- Stores discovered markets to ClickHouse market_windows table
- Runs discovery loop with configurable interval (default 5 min)
- Tracks known markets to avoid re-processing
- Graceful shutdown via broadcast channel

Key decisions:
- Using reqwest for HTTP instead of polymarket-client-sdk (simpler for read-only Gamma API)
- Gamma API returns clobTokenIds as JSON string, requires parsing
- 15-minute markets detected by title keywords (may need adjustment)
- Strike price parsing uses regex, defaults to Decimal::ZERO if not found
- Discovery interval set to 300s (5 minutes) in main.rs

Notes for next task (p1-4):
- Need tokio-tungstenite for Binance WebSocket
- Binance trade stream: wss://stream.binance.com:9443/ws/{symbol}@trade
- Buffer trades and batch write to spot_prices table

---

### [2026-01-12] Task p1-4: Binance WebSocket capture
Files created:
- crates/poly-collect/src/binance.rs

Files modified:
- crates/poly-collect/src/lib.rs (added binance module export)

Verification:
- cargo clippy -- -D warnings: PASSED
- cargo test -p poly-collect: PASSED (9 tests, 5 new)

BinanceCapture features:
- Connects to wss://stream.binance.com:9443/ws
- Subscribes to btcusdt@trade, ethusdt@trade, solusdt@trade, xrpusdt@trade
- Parses trade messages to SpotPrice (asset, price, quantity, timestamp)
- Buffers writes (default 500 records)
- Flushes to ClickHouse on buffer full or periodic interval (5s)
- Automatic reconnection with exponential backoff (1s initial, 60s max)
- Graceful shutdown via broadcast channel
- Handles ping/pong for connection keepalive
- Stats logging (trades received/written/errors)

Key decisions:
- Using combined stream subscription (single WS connection for all symbols)
- Decimal parsing from Binance string prices for exact math
- tokio::select! for concurrent message handling, flush timer, and shutdown
- Buffer cleared on successful flush, kept on error for retry
- Connection timeout of 10 seconds

Notes for next task (p1-5):
- Polymarket CLOB WebSocket at wss://clob.polymarket.com/ws
- Need to subscribe to token IDs from market discovery
- Handle book snapshots and deltas separately
- Periodic snapshot capture (100ms) for backtest fidelity

---

### [2026-01-12] Task p1-5: Polymarket CLOB WebSocket capture
Files created:
- crates/poly-collect/src/clob.rs

Files modified:
- crates/poly-collect/src/lib.rs (added clob module export)
- crates/poly-collect/Cargo.toml (added rust_decimal_macros dev dependency)

Verification:
- cargo clippy -- -D warnings: PASSED
- cargo test -p poly-collect: PASSED (19 tests, 10 new)

ClobCapture features:
- Connects to wss://ws-subscriptions-clob.polymarket.com/ws/market
- Subscribes to token IDs from shared ActiveMarkets state
- Handles "book" messages (full orderbook snapshots)
- Handles "price_change" messages (incremental deltas)
- Maintains in-memory OrderBookState per token with bid/ask HashMaps
- Periodic snapshot capture every 100ms for high-fidelity backtest data
- Buffers snapshots (500) and deltas (1000) for batch writes
- Automatic reconnection with exponential backoff (1s initial, 60s max)
- Sends PING every 9 seconds (Polymarket requires every 10s)
- Dynamically subscribes to new markets every 30 seconds
- Graceful shutdown via broadcast channel
- Stats logging (books received, snapshots/deltas captured/written)

OrderBookState features:
- apply_book() for full snapshot application
- apply_price_change() for delta updates (size=0 removes level)
- best_bid()/best_ask() for BBO extraction
- spread_bps() for spread calculation in basis points
- bid_depth()/ask_depth() for total liquidity
- to_snapshot() generates OrderBookSnapshot for ClickHouse

Key decisions:
- Using wss://ws-subscriptions-clob.polymarket.com/ws/market (official CLOB market endpoint)
- ActiveMarkets shared via Arc<RwLock<HashMap>> to coordinate with discovery
- Book messages parsed from JSON with bids/asks as OrderSummary arrays
- Price changes update both in-memory state AND record deltas for storage
- Snapshot interval of 100ms balances fidelity vs storage volume
- All prices use Decimal for exact financial math

Notes for next task (p1-6):
- Main binary needs to wire discovery, binance, and clob together
- ActiveMarkets shared between discovery (writer) and clob (reader)
- Need CLI args for --assets, --clickhouse-url
- Need TOML config file loading
- Graceful shutdown should stop all tasks

---

### [2026-01-12] Task p1-6: Collector main binary
Files created:
- crates/poly-collect/src/config.rs
- config/collect.toml

Files modified:
- Cargo.toml (added clap, toml workspace dependencies)
- crates/poly-collect/Cargo.toml (added clap, toml dependencies)
- crates/poly-collect/src/lib.rs (added config module export)
- crates/poly-collect/src/main.rs (complete rewrite)
- crates/poly-collect/src/discovery.rs (added discovered_windows tracking, get_discovered_windows method)

Verification:
- cargo clippy -- -D warnings: PASSED
- cargo test -p poly-collect: PASSED (23 tests, 4 new)

CollectConfig features:
- Top-level config struct with assets, discovery_interval, log_level, clickhouse, binance, clob, health_log_interval
- from_file() loads from TOML path
- from_toml_str() parses from string (for testing)
- apply_overrides() merges CLI args on top of config
- TOML structure with [general], [clickhouse], [binance], [clob], [health] sections

Main binary features:
- CLI argument parsing with clap (--config, --clickhouse-url, --assets)
- Config file loading with fallback to defaults
- ClickHouse connection test and table creation
- Shared ActiveMarkets (Arc<RwLock<HashMap>>) for discovery-CLOB coordination
- Broadcast shutdown channel for all tasks
- Spawns 4 async tasks:
  1. Discovery task with ActiveMarkets update on new markets
  2. Binance capture task
  3. CLOB capture task
  4. Health logging task
- Graceful shutdown:
  - Unix: SIGTERM and SIGINT handling
  - Windows: Ctrl+C handling
  - 10 second timeout for task completion
- Health stats: discovery_runs, markets_discovered, errors

Key decisions:
- Using clap derive for CLI argument parsing
- Config file is optional - defaults work out of the box
- Discovery now tracks discovered MarketWindows for CLOB subscription
- Separate spawn_* functions for cleaner task management
- Health stats use AtomicU64 for lock-free updates
- Broadcast channel capacity of 16 for shutdown signaling

Notes for next task (p1-7):
- config/collect.toml already created
- Need Dockerfile for poly-collect
- Need k8s Deployment manifest
- Consider adding health check HTTP endpoint

---

### [2026-01-12] Task p1-7: Deployment config
Files created:
- deploy/Dockerfile.collect
- deploy/k8s/poly-collect.yaml

Files modified:
- (none - config/collect.toml already existed from p1-6)

Verification:
- cargo clippy -- -D warnings: PASSED
- kubectl apply --dry-run=client: PASSED (namespace, configmap, deployment all valid)
- Docker build: SKIPPED (Docker daemon not running, but Dockerfile syntax is valid)

Dockerfile features:
- Multi-stage build (rust:1.83-bookworm builder, debian:bookworm-slim runtime)
- Minimal runtime with only ca-certificates and libssl3
- Non-root user (appuser) for security
- RUST_LOG and CONFIG_PATH environment defaults
- Copies default config to /app/config/collect.toml

K8s manifest features:
- Namespace: polymarket
- ConfigMap with embedded collect.toml (ClickHouse URL points to cluster-internal service)
- Deployment with:
  - Resource limits (128Mi-512Mi memory, 100m-500m CPU)
  - Security context (non-root, read-only root fs, dropped capabilities)
  - Config mounted from ConfigMap at /config
  - terminationGracePeriodSeconds: 15 for graceful shutdown
  - RUST_LOG and RUST_BACKTRACE env vars

Key decisions:
- Using debian:bookworm-slim for runtime (smaller than alpine, better glibc compatibility)
- Security-hardened pod spec with minimal privileges
- ConfigMap allows config changes without rebuilding image
- ClickHouse URL uses cluster DNS (clickhouse.polymarket.svc.cluster.local)
- Health check endpoint not added (would require code changes, can be added later)

Notes for Phase 1 completion:
- Phase 1 (poly-collect) is now COMPLETE!
- Can deploy to k8s once Docker daemon is running and ClickHouse is deployed
- Start collecting data ASAP to build backtest dataset

---

### [2026-01-12] Task p2-1: Price history importer
Files created:
- crates/poly-import/Cargo.toml
- crates/poly-import/src/lib.rs
- crates/poly-import/src/prices.rs
- crates/poly-import/src/main.rs (stub)

Files modified:
- Cargo.toml (added poly-import to workspace members)
- crates/poly-common/src/types.rs (added PriceHistory, TradeHistory types)
- crates/poly-common/src/clickhouse.rs (added price_history/trade_history inserters)

Verification:
- cargo clippy -- -D warnings: PASSED
- cargo test: PASSED (35 tests total, 4 new)

PriceImporter features:
- Fetches from https://clob.polymarket.com/prices-history endpoint
- Supports startTs/endTs for date range filtering
- Supports fidelity parameter for resolution control
- Rate limiting with configurable requests_per_second (default 5.0)
- Automatic retry with exponential backoff (3 retries, 1s initial)
- Handles 429 Too Many Requests with Retry-After header
- Batch inserts to ClickHouse (configurable batch_size, default 1000)
- Import stats tracking (tokens processed/failed, records imported)
- Multi-token import with progress logging

API response format:
- GET /prices-history?market={token_id}&startTs={unix}&endTs={unix}&fidelity={minutes}
- Returns: { history: [{ t: unix_timestamp, p: price_float }] }

Types added to poly-common:
- PriceHistory (token_id, timestamp, price) for price_history table
- TradeHistory (token_id, timestamp, side, price, size, trade_id) for trade_history table

Key decisions:
- Using reqwest for HTTP client (consistent with poly-collect)
- Conservative rate limit of 5 req/s to avoid 429s
- API returns f64 prices, converted to Decimal on import
- CLI not implemented yet (deferred to p2-3)
- TradeHistory type added preemptively for p2-2

Notes for next task (p2-2):
- Need to find trade history API endpoint
- TradeHistory type already defined
- insert_trade_history already implemented in ClickHouse client

---
